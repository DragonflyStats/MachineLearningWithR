%% -  http://www.chromedia.org/chromedia?waxtrapp=ksckzDsHonOvmOlIEcCxBW&subNav=qaphrDsHonOvmOlIEcCxBWR

%% - http://www.rsc.org/images/robust-statistics-technical-brief-6_tcm18-214850.pdf

\documentclass[12pt, a4paper]{article}

\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 470pt

\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\usepackage{framed}
\bibliographystyle{chicago}



\begin{document}
	\author{Kevin O'Brien}
	\title{MA4605}
	
	\tableofcontents \setcounter{tocdepth}{2}
	%-------------------------------------------------
\newpage
\Large	
\section{Introduction to Analysis with \texttt{R} }

\subsection{Measures of Dispersion}

Recall:

\begin{itemize}
	
	\item standard deviation  = square root of variance
	
	\item variance = squared standard deviation
	
	\item coefficient of variation = relative standard deviation (in
	
	percentage)
	
\end{itemize}


\subsection{Titration experiment}



Recall the titration experiment from the last class. 4 Students performing the same experiment five times, hence each yield 5 results.(Table 1.1 random and systematic errors).

\begin{tabular}{|c|ccccc|l|}
	
	\hline
	
	% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
	
	Student & Results  & (ml) &  &  &  &Comment \\ \hline
	
	A & 10.08 & 10.11 &10.09 &10.10&10.12 & Precise, biased\\ \hline
	
	B & 9.88 &10.14& 10.02 &9.80& 10.21& Imprecise unbiased\\ \hline
	
	C & 10.19 &9.79& 9.69 &10.05& 9.78 & Imprecise, biased\\ \hline
	
	D & 10.04 &9.98 &10.02 &9.97 &10.04 & Precise, unbiased \\
	
	\hline
	
\end{tabular}\\











Two criteria were used to compare these results, the average value (technically know

as a measure of location and the degree of spread (or dispersion). The average value

used was the arithmetic mean (usually abbreviated to \emph{the mean}), which is the sum

of all the measurements divided by the number of measurements.





The mean,$\bar{X}$, of $n$ measurements is given by \[ \bar{X}  = {\sum{x} \over n} \]



In Chapter 1 the spread was measured by the difference between the highest and

lowest values (i.e. the range). A more useful measure, which utilizes all the values, is the sample

standard deviation, $s$, which is defined as follows:



The standard deviation, $s$, of $n$ measurements is given by

\[s=  \sqrt{ {\sum(x-\bar{X})^2 \over n-1} }  (2.2) \]


\subsection{Means and standard deviations using \texttt{R}}

\subsection{Bias and precision using mean and standard deviation}



Classify bias and precision using means and standard deviation

of measurements.

\begin{framed}
	\begin{verbatim}
	
	#Comuting means
	
	rowMeans(Titra)
	
	# A B C D
	
	#10.0950 9.9600 9.9300 10.0025
	
	#and standard deviation
	
	apply(Titra,1,sd)
	
	# A B C D
	
	#0.01290994 0.15055453 0.23036203 0.03304038
	
	\end{verbatim}
\end{framed}
%===================================================================%
\newpage
\section{Testing Normality}

An assessment of the normality of data is a prerequisite for many statistical tests as normal data is an underlying assumption in parametric testing. There are two main methods of assessing normality - graphically and numerically.




Importantly, Hypothesis tests are tests against normality. Correctly we are not determining whether a data set is normally distributed or not.
What we are doing is testing to see if there is enough evidence to classify the data as non-normal. If not, we rely on the assumption of normality.
Just because we can't demonstrate that the data is not normally distributed, does not mean that the data is normally distributed.
Using multiple procedures in conjunction is advised for getting a proper assessment of the data.

\begin{itemize}
	\item Shapiro Wilk Test
	\item Anderson-Darling Test 
	\item Normal Probability Plot
\end{itemize}

\newpage

\section{Testing Normality}
An assessment of the normality of data is a prerequisite for many statistical tests as normal data is an underlying assumption in parametric testing. There are two main methods of assessing normality - graphically and numerically.


\subsection{Testing for normality of distribution}
The chi-square test could be used for testing normality by
dividing the range of data into bins and compare the count in
each bin with the corresponding probabilities based on the
normal distribution.
Unfortunately, one needs relatively large data sample size in
order to use chi-squared test ($> 50$), thus there is a need for a
small sample size procedure.

\subsection{Normaly Probability Plots}
%pg 37-39
Normal probability plots
One simple graphical way of comparing data to normal
distributions is by plotting empirical quantile vs.
corresponding normal quantile.
Recall that the p-quantile for a given (empirical) distribution
is the number below of which there is 100p% of probability
(of the data).
Consider data from Example 3.12.1
x=c(109,89,99,99,107,111,86,74,115,107,134,113,110,88,104)
The normal probability plot is obtained in R using
qqnorm(x)

%pg 40
\subsection{KS test}

%pg 41

Kolmogorov-Smirnov test in R
\begin{verbatim}
x=c(25.13,25.02,25.11,25.07,25.03,24.97,25.14,25.09)
ks.test(x,"pnorm",mean(x),sd(x))
One-sample Kolmogorov-Smirnov test
data: x
D = 0.1321, p-value = 0.995
alternative hypothesis: two-sided
\end{verbatim}


Bivariate Analysis

Correlation


\newpage
Robust statistics Statistical methods insensitive to the effects of outliers (which may be mistakes or contaminated data). The methods rely on medians rather than means, and use more information from the central than from the outlying observations. The ideas are associated with exploratory data analysis.

The median is a robust measure of central tendency, while the mean is not. The median has a breakdown point of 50%, while the mean has a breakdown point of 0% (a single large observation can throw it off).
The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not.
Trimmed estimators and Winsorised estimators are general methods to make statistics more robust. 

\end{document}

