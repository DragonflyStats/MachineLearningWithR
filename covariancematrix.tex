a covariance matrix (also known as dispersion matrix or variance–covariance matrix) is a matrix whose element in the i, j position is the covariance between the i th and j th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable. Each element has either a finite number of observed empirical values or a finite or infinite number of potential values. The potential values are specified by a theoretical joint probability distribution.

Intuitively, the covariance matrix generalizes the notion of covariance to multiple dimensions. As an example, let's consider two vectors {\displaystyle X=[x_{1},x_{2}]^{T}} {\displaystyle X=[x_{1},x_{2}]^{T}} and {\displaystyle Y=[y_{1},y_{2}]^{T}} {\displaystyle Y=[y_{1},y_{2}]^{T}}. There are four covariances to consider: {\displaystyle x_{1}} x_{1} with {\displaystyle y_{1}} y_{1}, {\displaystyle x_{1}} x_{1} with {\displaystyle y_{2}} y_{2}, {\displaystyle x_{2}} x_{2} with {\displaystyle y_{1}} y_{1}, and {\displaystyle x_{2}} x_{2} with {\displaystyle y_{2}} y_{2}. These variances cannot be summarized in a scalar. Of course, a 2×2 matrix is the most natural choice to describe the covariance: the first row containing the covariances of {\displaystyle x_{1}} x_{1} with {\displaystyle y_{1}} y_{1} and {\displaystyle y_{2}} y_{2}, and the second row containing the covariances of {\displaystyle x_{2}} x_{2} with {\displaystyle y_{1}} y_{1} and {\displaystyle y_{2}} y_{2}.

Because the covariance of the i th random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is just the variance of each of the elements in the vector. Because {\displaystyle {\text{Covar}}(x_{i},x_{j})={\text{Covar}}(x_{j},x_{i})} {\displaystyle {\text{Covar}}(x_{i},x_{j})={\text{Covar}}(x_{j},x_{i})}, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.
