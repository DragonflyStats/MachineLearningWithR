\documentclass[]{article}


\begin{document}
	\tableofcontents
	\newpage
	\section{Performance of Classification Procedure}
	
	These classifications are used to calculate accuracy, precision (also called positive predictive value), recall (also called sensitivity), specificity and negative predictive value:
	
	\begin{itemize}
		\item  \textbf{Accuracy} is the fraction of observations with correct predicted classification
		\[ \mbox{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN}\]
		
		
		\item \textbf{Precision} is the proportion of predicted positives that are correct
		\[
		\mbox{Precision} = \mbox{Positive Predictive Value} =\frac{TP}{TP+FP} \, \]
		
		\item \textbf{Negative Predictive Value} is the  fraction of predicted negatives that are correct
		\[\mbox{Negative Predictive Value} = \frac{TN}{TN+FN}\]
		
		\item \textbf{Recall} is the fraction of observations that are actually 1 with a correct predicted classification
		\[ 
		\mbox{Recall} = \mbox{Sensitivity} = \frac{TP}{TP+FN} \,  \]
		
		\item \textbf{Specificity} is the fraction of observations that are actually 0 with a correct predicted classification
		\[ \mbox{Specificity} = \frac{TN}{TN+FP} \]
		
	\end{itemize}

%================================================================================== %
\newpage	
\section{Machine learning: the problem setting}
	
	\noindent In general, a learning problem considers a set of $n$ samples of data and try to predict properties of unknown data. If
	each sample is more than a single number, and for instance a multi-dimensional entry (aka multivariate data), is it said
	to have several variables, also known as attributes or \textbf{\textit{features}}.\\
	\bigskip
	We can separate learning problems in a few large categories:
	
	\begin{itemize}
		\item \textbf{Supervised learning}, in which the data comes with additional attributes that we want to predict.\\ 
		%(Click here to go to the Scikit-Learn supervised learning page).
		\bigskip
		This problem can be either:
		\begin{description}
			\item[Classification:] samples belong to two or more classes and we want to learn from already labeled data how
			to predict the class of unlabeled data. \\ An example of classification problem would be the digit recognition
			example, in which the aim is to assign each input vector to one of a finite number of discrete categories.
			\item[Regression:] if the desired output consists of one or more continuous variables, then the task is called
			regression. \\ An example of a regression problem would be the prediction of the weight of a pony as a
			function of its age and height.
		\end{description}
		
		\newpage
		\item  \textbf{Unsupervised learning}, in which the training data consists of a set of input vectors $x$ without any corresponding
		target values. \\ \bigskip The goal in such problems may be
		\begin{itemize}
			\item to discover groups of similar examples within the data, where
			it is called \textbf{\textit{clustering}}, \item to determine the distribution of data within the input space, known as \textbf{\textit{density estimation}},
			\item to project the data from a high-dimensional space down to two or thee dimensions for the purpose of
			visualization 
		\end{itemize}
		% (Click here to go to the Scikit-Learn unsupervised learning page).
	\end{itemize}
	
	%======================================================================== %
\newpage

\section{Training and validation}
%http://www.jmp.com/support/help/Validation_2.shtml
Using Validation and Test Data

%When you have sufficient data, you can subdivide your data into three parts called the training, validation, and test data. During the selection process, models are fit on the training data, and the prediction error for the models so obtained is found by using the validation data. This prediction error on the validation data can be used to decide when to terminate the selection process or to decide what effects to include as the selection process proceeds. Finally, once a selected model has been obtained, the test set can be used to assess how the selected model generalizes on data that played no role in selecting the model.

In some cases you might want to use only training and test data. For example, you might decide to use an information criterion to decide what effects to include and when to terminate the selection process. In this case no validation data are required, but test data can still be useful in assessing the predictive performance of the selected model. In other cases you might decide to use validation data during the selection process but forgo assessing the selected model on test data. 
%Hastie, Tibshirani, and Friedman (2001) note that it is difficult to give a general rule on how many observations you should assign to each role. They note that a typical split might be 50\% for training and 25% each for validation and testing.



%---------------------------%
\section{Supervised learning}


Supervised learning is the machine learning task of inferring a function from supervised training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which is called a classifier (if the output is discrete, see classification) or a regression function (if the output is continuous, see regression). The inferred function should predict the correct output value for any valid input object. This requires the learning algorithm to generalize from the training data to unseen situations 

in a "reasonable" way (see inductive bias). 



\newpage
% 2. Supervised learning: predicting an output variable from high-dimensional observations

The problem solved in supervised learning

Supervised learning consists in learning the link between two datasets: the observed data X, and an external variable y that we are trying to predict, usually called target or labels. Most often, y is a 1D array of length \texttt{n\_samples}.

All supervised estimators in the scikit-learn implement a fit(X, y) method to fit the model, and a predict(X) method that, given unlabeled observations X, returns predicts the corresponding labels y.
%================================================================================ %
Vocabulary: classification and regression

If the prediction task is to classify the observations in a set of finite labels, in other words to “name” the objects observed, the task is said to be a classification task. On the opposite, if the goal is to predict a continous target variable, it is said to be a regression task.

In the scikit-learn, for classification tasks, y is a vector of integers.

%================================================================================ %
2.1. Nearest neighbor and the curse of dimensionality

\subsection{Classifying irises:}

The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:

\begin{framed}
\begin{verbatim}
>>> import numpy as np
>>> from scikits.learn import datasets
>>> iris = datasets.load_iris()
>>> iris_X = iris.data
>>> iris_y = iris.target
>>> np.unique(iris_y)
array([0, 1, 2])
\end{verbatim}
\end{framed}
%============================================================================= %
\subsubsection{k-Nearest neigbhors classifier}

The simplest possible classifier is the nearest neighbor: given a new observation \texttt{x\_test}, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector.
%============================================================================== %
\end{document}
